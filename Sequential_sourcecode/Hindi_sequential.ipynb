{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1ec5a89-f71d-4b1d-80e2-5a8933b8eb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, GlobalMaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f933df6d-5253-450c-b66b-cd3ad57633c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 212ms/step - accuracy: 0.5223 - loss: 0.5045 - val_accuracy: 0.6634 - val_loss: 0.3155\n",
      "Epoch 2/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 218ms/step - accuracy: 0.6666 - loss: 0.3107 - val_accuracy: 0.6806 - val_loss: 0.2848\n",
      "Epoch 3/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 225ms/step - accuracy: 0.6966 - loss: 0.2600 - val_accuracy: 0.6893 - val_loss: 0.2582\n",
      "Epoch 4/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 229ms/step - accuracy: 0.7302 - loss: 0.2150 - val_accuracy: 0.7115 - val_loss: 0.2473\n",
      "Epoch 5/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 230ms/step - accuracy: 0.7765 - loss: 0.1735 - val_accuracy: 0.7238 - val_loss: 0.2649\n",
      "Epoch 6/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 220ms/step - accuracy: 0.7999 - loss: 0.1570 - val_accuracy: 0.7250 - val_loss: 0.2752\n",
      "Epoch 7/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 226ms/step - accuracy: 0.8212 - loss: 0.1393 - val_accuracy: 0.7115 - val_loss: 0.2846\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - accuracy: 0.7109 - loss: 0.2649\n",
      "Test Loss: 0.2498\n",
      "Test Accuracy: 0.7181\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  defamation       0.00      0.00      0.00       169\n",
      "        fake       0.67      0.65      0.66       334\n",
      "        hate       0.41      0.16      0.23       234\n",
      " non-hostile       0.94      0.96      0.95       873\n",
      "   offensive       0.59      0.47      0.52       219\n",
      "\n",
      "   micro avg       0.81      0.65      0.72      1829\n",
      "   macro avg       0.52      0.45      0.47      1829\n",
      "weighted avg       0.70      0.65      0.67      1829\n",
      " samples avg       0.69      0.70      0.69      1829\n",
      "\n",
      "Predicted percentages for the first test instance:\n",
      "defamation: 16.87%\n",
      "fake: 47.91%\n",
      "hate: 27.46%\n",
      "non-hostile: 1.45%\n",
      "offensive: 20.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "train_dataset = pd.read_excel('E://PROJECTMINIS/DATASETS//hindi dataset//constraint_Hindi_Train.xlsx',engine='openpyxl')\n",
    "val_dataset = pd.read_excel('E://PROJECTMINIS//DATASETS//hindi dataset//constraint_Hindi_Valid.xlsx',engine='openpyxl')\n",
    "test_dataset = pd.read_excel('E://PROJECTMINIS//DATASETS//hindi dataset//Test Set Complete.xlsx',engine='openpyxl')\n",
    "val_dataset.rename(columns={'Labels Set': 'labels'}, inplace=True)\n",
    "train_dataset.rename(columns={'Labels Set': 'labels'}, inplace=True)\n",
    "test_dataset.rename(columns={'Labels Set': 'labels'}, inplace=True)\n",
    "train_dataset.rename(columns={'Post': 'text'}, inplace=True)\n",
    "val_dataset.rename(columns={'Post': 'text'}, inplace=True)\n",
    "test_dataset.rename(columns={'Post': 'text'}, inplace=True)\n",
    "# Assuming 'labels' column contains comma-separated labels, e.g., \"hostile,fake\"\n",
    "train_dataset['labels'] = train_dataset['labels'].apply(lambda x: x.split(','))\n",
    "val_dataset['labels'] = val_dataset['labels'].apply(lambda x: x.split(','))\n",
    "test_dataset['labels'] = test_dataset['labels'].apply(lambda x: x.split(','))\n",
    "\n",
    "# MultiLabelBinarizer to convert string labels into binary matrix (0 or 1 for each label)\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train = mlb.fit_transform(train_dataset['labels'])\n",
    "y_val = mlb.transform(val_dataset['labels'])\n",
    "y_test = mlb.transform(test_dataset['labels'])\n",
    "\n",
    "# Tokenization and preprocessing\n",
    "max_words = 10000\n",
    "max_sequence_length = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_dataset['text'])\n",
    "\n",
    "# Convert texts to sequences\n",
    "X_train = tokenizer.texts_to_sequences(train_dataset['text'])\n",
    "X_val = tokenizer.texts_to_sequences(val_dataset['text'])\n",
    "X_test = tokenizer.texts_to_sequences(test_dataset['text'])\n",
    "\n",
    "# Pad sequences to ensure uniform input size\n",
    "X_train_pad = pad_sequences(X_train, maxlen=max_sequence_length)\n",
    "X_val_pad = pad_sequences(X_val, maxlen=max_sequence_length)\n",
    "X_test_pad = pad_sequences(X_test, maxlen=max_sequence_length)\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(Embedding(input_dim=max_words, output_dim=100, input_length=max_sequence_length))\n",
    "\n",
    "# LSTM layer for sequence learning\n",
    "model.add(LSTM(100, activation='tanh', return_sequences=True))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# Dense layer\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Output layer: one output for each label, with sigmoid activation\n",
    "model.add(Dense(len(mlb.classes_), activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to avoid overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_pad, y_train, validation_data=(X_val_pad, y_val), epochs=10, batch_size=64, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test_pad, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Generate predictions for the test data\n",
    "test_predictions = model.predict(X_test_pad)\n",
    "\n",
    "# Convert the probabilities to binary values (using a 0.5 threshold)\n",
    "test_predictions_binary = (test_predictions > 0.5).astype(int)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, test_predictions_binary, target_names=mlb.classes_))\n",
    "\n",
    "# Optionally, print predicted probabilities for the first test instance (percentage confidence for each label)\n",
    "print(\"Predicted percentages for the first test instance:\")\n",
    "for label, prob in zip(mlb.classes_, test_predictions[0]):\n",
    "    print(f\"{label}: {prob*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b838c5e3-6067-4f17-a72d-67f99d7d6025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "Text: ये दलित लोग समाज का कचरा हैं, इन्हें गाँव से भगा देना चाहिए।\n",
      "Predicted Categories and Percentages:\n",
      "defamation: 3.36%\n",
      "fake: 73.85%\n",
      "hate: 3.94%\n",
      "non-hostile: 17.17%\n",
      "offensive: 1.43%\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Text: मुसलमानों को मार डालो, ये सब आतंकवादी हैं जो हिंदुस्तान को तबाह कर देंगे!\n",
      "Predicted Categories and Percentages:\n",
      "defamation: 30.61%\n",
      "fake: 13.73%\n",
      "hate: 47.82%\n",
      "non-hostile: 0.35%\n",
      "offensive: 51.73%\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Text: पंजाबी लोग सिर्फ पैसा कमाने के लिए फर्जी दवाइयाँ बेचते हैं, ये लालची कुत्ते सबको लूट रहे हैं।\n",
      "Predicted Categories and Percentages:\n",
      "defamation: 27.82%\n",
      "fake: 20.05%\n",
      "hate: 39.51%\n",
      "non-hostile: 0.45%\n",
      "offensive: 42.07%\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Text: : \"हरियाणा के लोग बहुत मेहनती हैं, उनके खेतों से देश को इतना अनाज मिलता है\n",
      "Predicted Categories and Percentages:\n",
      "defamation: 20.33%\n",
      "fake: 39.66%\n",
      "hate: 27.25%\n",
      "non-hostile: 0.84%\n",
      "offensive: 23.49%\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a function to preprocess custom inputs\n",
    "def preprocess_input(texts, tokenizer, max_sequence_length=100):\n",
    "    # Tokenize the custom input texts\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    # Pad the sequences to ensure the same input shape as training data\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "    return padded_sequences\n",
    "\n",
    "# Example custom inputs (these can be any sentences you want to classify)\n",
    "custom_texts = [\n",
    " \"ये दलित लोग समाज का कचरा हैं, इन्हें गाँव से भगा देना चाहिए।\",\n",
    "    \"मुसलमानों को मार डालो, ये सब आतंकवादी हैं जो हिंदुस्तान को तबाह कर देंगे!\",\n",
    "    'पंजाबी लोग सिर्फ पैसा कमाने के लिए फर्जी दवाइयाँ बेचते हैं, ये लालची कुत्ते सबको लूट रहे हैं।',\n",
    "    ': \"हरियाणा के लोग बहुत मेहनती हैं, उनके खेतों से देश को इतना अनाज मिलता है'\n",
    "]\n",
    "\n",
    "# Preprocess the custom input texts\n",
    "custom_input_data = preprocess_input(custom_texts, tokenizer)\n",
    "\n",
    "# Get predictions for the custom inputs\n",
    "predictions = model.predict(custom_input_data)\n",
    "\n",
    "# Display the predicted probabilities for each label\n",
    "for i, text in enumerate(custom_texts):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"Predicted Categories and Percentages:\")\n",
    "    for label, prob in zip(mlb.classes_, predictions[i]):\n",
    "        print(f\"{label}: {prob * 100:.2f}%\")\n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6055edbd-f374-4fdf-8057-9986bf1d795a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hindi_model.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, 'hindi_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2845877-4236-4720-935a-12c04d1b2fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved to tokenizer.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "\n",
    "# Save the tokenizer as a .pkl file\n",
    "with open('tokenizer_hindi.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "print(\"Tokenizer saved to tokenizer.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.9",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
