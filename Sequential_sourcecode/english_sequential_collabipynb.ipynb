{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d48Bv81a7MSN",
    "outputId": "6b1bd3fe-ff23-467e-d242-badb46460973"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  label\n",
      "4161   South Africa Charges 4 Suspected of Plotting t...    0.0\n",
      "4162                                   terrorist leftist    1.0\n",
      "4163   RT HanzalaOfficial: India your link with ISIS ...    0.0\n",
      "4164   Seize Pope & Rothschild who owns Fed Reserve  ...    0.0\n",
      "4165   Saudi-led bombing in Yemen; ISIS abuses; Turke...    0.0\n",
      "...                                                  ...    ...\n",
      "57302  Russian Forces Continue March into Syrian Isla...    0.0\n",
      "57303  EL #Califato es la reaparici  n de una ideolog...    0.0\n",
      "57304  @SilentSecretMan reporte toi    la s  rie \"The...    1.0\n",
      "57305  @Dabiq_Warrior @Totenleserin Countryside will ...    1.0\n",
      "57306  @A_Moon_Banana @Mai_svg see I could say the sa...    0.0\n",
      "\n",
      "[53146 rows x 2 columns]\n",
      "                                                     text  label\n",
      "0       denial of normal the con be asked to comment o...      1\n",
      "1       just by being able to tweet this insufferable ...      1\n",
      "2       that is retarded you too cute to be single tha...      1\n",
      "3       thought of a real badass mongol style declarat...      1\n",
      "4                                     afro american basho      1\n",
      "...                                                   ...    ...\n",
      "726114  i mute this telecasting and played kanye west ...      1\n",
      "726115  but hell yeah he s not a bachelor but looooooo...      1\n",
      "726116  great video musician but s not my musician lol...      1\n",
      "726117  not great pop video yeah he s not a pedophile ...      1\n",
      "726118  great video yeah he s non a paedophile lolllll...      1\n",
      "\n",
      "[726119 rows x 2 columns]\n",
      "                                                    text  label\n",
      "0      thirtysomething scientists unveil doomsday clo...      1\n",
      "1      dem rep. totally nails why congress is falling...      0\n",
      "2      eat your veggies: 9 deliciously different recipes      0\n",
      "3      inclement weather prevents liar from getting t...      1\n",
      "4      mother comes pretty close to using word 'strea...      1\n",
      "...                                                  ...    ...\n",
      "28614       jews to celebrate rosh hashasha or something      1\n",
      "28615  internal affairs investigator disappointed con...      1\n",
      "28616  the most beautiful acceptance speech this week...      0\n",
      "28617  mars probe destroyed by orbiting spielberg-gat...      1\n",
      "28618                 dad clarifies this not a food stop      1\n",
      "\n",
      "[28619 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Simulated datasets (replace with your actual datasets)\n",
    "# Dataset1: Hate speech (0: non-hate, 1: hate)\n",
    "dataset1 = pd.read_csv('MultiLanguageTrainDataset.csv')\n",
    "dataset1 = dataset1[dataset1['language'] == 2]\n",
    "dataset1 = dataset1.drop(columns=['Unnamed: 0'])\n",
    "dataset1 = dataset1.drop(columns=['language'])\n",
    "print(dataset1)\n",
    "\n",
    "dataset2 = pd.read_csv('HateSpeechDatasetBalanced.csv')\n",
    "dataset2.rename(columns={'Content': 'text'}, inplace=True)\n",
    "dataset2.rename(columns={'Label': 'label'}, inplace=True)\n",
    "print(dataset2)\n",
    "\n",
    "\n",
    "\n",
    "# Dataset3: Sarcasm (0: non-sarcasm, 1: sarcasm)\n",
    "dataset3 = pd.read_json('Sarcasm_Headlines_Dataset_v2.json',lines=True)\n",
    "dataset3 = dataset3.drop(columns=['article_link'])\n",
    "dataset3.rename(columns={'headline': 'text'}, inplace=True)\n",
    "dataset3.rename(columns={'is_sarcastic': 'label'}, inplace=True)\n",
    "dataset3 = dataset3[['text', 'label']]\n",
    "print(dataset3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oENCji9rCZbN",
    "outputId": "3a5bca47-715d-40d2-b583-4576426bffdc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m16158/16158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 9ms/step - accuracy: 0.7953 - loss: 0.4482 - val_accuracy: 0.8435 - val_loss: 0.3484\n",
      "Epoch 2/5\n",
      "\u001b[1m16158/16158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 8ms/step - accuracy: 0.8614 - loss: 0.3166 - val_accuracy: 0.8577 - val_loss: 0.3255\n",
      "Epoch 3/5\n",
      "\u001b[1m16158/16158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 8ms/step - accuracy: 0.8851 - loss: 0.2690 - val_accuracy: 0.8635 - val_loss: 0.3176\n",
      "Epoch 4/5\n",
      "\u001b[1m16158/16158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 9ms/step - accuracy: 0.9032 - loss: 0.2304 - val_accuracy: 0.8633 - val_loss: 0.3290\n",
      "Epoch 5/5\n",
      "\u001b[1m16158/16158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 8ms/step - accuracy: 0.9207 - loss: 0.1939 - val_accuracy: 0.8632 - val_loss: 0.3412\n",
      "\u001b[1m5050/5050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Normal Speech       0.87      0.86      0.87     81080\n",
      "  Hate Speech       0.86      0.87      0.87     77703\n",
      "      Sarcasm       0.71      0.69      0.70      2794\n",
      "\n",
      "     accuracy                           0.86    161577\n",
      "    macro avg       0.81      0.81      0.81    161577\n",
      " weighted avg       0.86      0.86      0.86    161577\n",
      "\n",
      "\n",
      "Predictions for Input Texts:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\n",
      "Text: 'I am so happy with the results!'\n",
      "Predicted: Normal Speech (Class 0)\n",
      "Normal Speech: 96.58%\n",
      "Hate Speech: 3.42%\n",
      "Sarcasm: 0.00%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\n",
      "Text: 'I hate you!'\n",
      "Predicted: Hate Speech (Class 1)\n",
      "Normal Speech: 42.39%\n",
      "Hate Speech: 57.61%\n",
      "Sarcasm: 0.00%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\n",
      "Text: 'I hate Chinese people!'\n",
      "Predicted: Hate Speech (Class 1)\n",
      "Normal Speech: 17.85%\n",
      "Hate Speech: 82.14%\n",
      "Sarcasm: 0.00%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "Text: 'That movie was just amazing, I totally get the sarcasm!'\n",
      "Predicted: Hate Speech (Class 1)\n",
      "Normal Speech: 7.79%\n",
      "Hate Speech: 92.21%\n",
      "Sarcasm: 0.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Combine datasets\n",
    "# Map labels: 0 (Normal Speech), 1 (Hate Speech), 2 (Sarcasm)\n",
    "dataset1['label'] = dataset1['label'].map({0: 0, 1: 1})  # Non-hate -> Normal, Hate -> Hate\n",
    "dataset2['label'] = dataset2['label'].map({0: 0, 1: 1})  # Non-hate -> Normal, Hate -> Hate\n",
    "dataset3['label'] = dataset3['label'].map({0: 0, 1: 2})  # Non-sarcasm -> Normal, Sarcasm -> Sarcasm\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([dataset1, dataset2, dataset3], ignore_index=True)\n",
    "\n",
    "# Shuffle the data\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Parameters\n",
    "max_words = 10000  # Maximum number of words to consider\n",
    "max_sequence_length = 100  # Maximum length of sequences\n",
    "embedding_dim = 100  # Dimension of word embeddings\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data['text'])\n",
    "sequences = tokenizer.texts_to_sequences(data['text'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(data['label'])\n",
    "labels = to_categorical(labels, num_classes=3)  # One-hot encode for 3 classes\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build model\n",
    "model = Sequential([\n",
    "    Embedding(max_words, embedding_dim, input_length=max_sequence_length),\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # 3 classes: Normal, Hate, Sarcasm\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate model\n",
    "test_predictions = model.predict(X_test)\n",
    "predictions_classes = np.argmax(test_predictions, axis=1)\n",
    "test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Classification report\n",
    "class_names = ['Normal Speech', 'Hate Speech', 'Sarcasm']\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, predictions_classes, target_names=class_names))\n",
    "\n",
    "# Modified prediction function to show all class probabilities\n",
    "def predict_input_with_percentage(text, tokenizer, model, max_sequence_length=100):\n",
    "    sequences = tokenizer.texts_to_sequences([text])\n",
    "    padded_sequence = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "    # Predict probabilities for each class\n",
    "    prediction = model.predict(padded_sequence)[0]\n",
    "\n",
    "    # Get the class with the highest probability\n",
    "    predicted_class = np.argmax(prediction)\n",
    "\n",
    "    # Get probabilities for all classes\n",
    "    probabilities = prediction * 100\n",
    "\n",
    "    return predicted_class, probabilities\n",
    "\n",
    "# Example input texts\n",
    "input_texts = [\n",
    "    \"I am so happy with the results!\",\n",
    "    \"I hate you!\",\n",
    "    \"I hate Chinese people!\",\n",
    "    \"That movie was just amazing, I totally get the sarcasm!\"\n",
    "]\n",
    "\n",
    "# Test each input and display probabilities for all classes\n",
    "print(\"\\nPredictions for Input Texts:\")\n",
    "for text in input_texts:\n",
    "    predicted_class, probabilities = predict_input_with_percentage(text, tokenizer, model, max_sequence_length)\n",
    "    print(f\"\\nText: '{text}'\")\n",
    "    print(f\"Predicted: {class_names[predicted_class]} (Class {predicted_class})\")\n",
    "    for class_name, prob in zip(class_names, probabilities):\n",
    "        print(f\"{class_name}: {prob:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YsAdDg6p9ktD",
    "outputId": "3697b479-b834-4abf-977d-bfa8f82718f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['english_model_collab.pkl']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, 'english_model_collab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUeEIbMs96Xw",
    "outputId": "8b761c81-8b68-4aed-c089-7f311508a493"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved to tokenizer.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "\n",
    "# Save the tokenizer as a .pkl file\n",
    "with open('tokenizer_english_collab.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "print(\"Tokenizer saved to tokenizer.pkl\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
